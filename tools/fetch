#!/usr/bin/env -S uv run --script
import hashlib
import json
import csv
import subprocess
import asyncio
from pathlib import Path
import aiohttp
import typer
from bs4 import BeautifulSoup
from loguru import logger
from typing_extensions import Annotated

from apt.config import Config

app = typer.Typer()

def get_download_url(page: str) -> str:
    soup = BeautifulSoup(page, 'lxml')
    scripts = soup.find('body').find_all('script')
    sections = scripts[-1].contents[0].split(';')
    app_api = json.loads(sections[0].split('=')[1])['/app-api/enduserapp/shared-item']

    box_url = "https://app.box.com/index.php"
    box_args = "?rm=box_download_shared_file&shared_name={}&file_id={}"
    file_url = box_url + box_args.format(app_api['sharedName'], f"f_{app_api['itemID']}")

    return file_url

def report_already_downloaded(download_path: Path) -> bool:
    if download_path.exists():
        return True
    if download_path.with_suffix('.pdf').exists():
        return True
    return False

async def download_report(session: aiohttp.ClientSession, report: dict, output_base: Path, semaphore: asyncio.Semaphore) -> tuple[str, bool]:
    report_year = str(report['Year'])
    report_filename = report['Filename']
    report_link = report['Link']
    report_sha1 = report['SHA-1']

    async with semaphore:
        try:
            year_dir = output_base / report_year
            year_dir.mkdir(parents=True, exist_ok=True)

            download_path = year_dir / report_filename

            if report_already_downloaded(download_path):
                return ('skip', True)

            logger.info(f"Downloading {report_filename} ({report_year})")

            # Get the splash page to extract download URL
            async with session.get(report_link, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                report_splash = await resp.text()

            file_url = get_download_url(report_splash)

            hash_check = hashlib.sha1()

            # Download the actual file
            async with session.get(file_url, timeout=aiohttp.ClientTimeout(total=120)) as resp:
                resp.raise_for_status()

                with open(download_path, 'wb') as f:
                    async for chunk in resp.content.iter_chunked(8192):
                        if chunk:
                            f.write(chunk)
                            hash_check.update(chunk)

            if hash_check.hexdigest() != report_sha1:
                download_path.unlink()
                raise ValueError(f"SHA-1 mismatch for {report_filename}")

            if not download_path.suffix:
                pdf_path = download_path.with_suffix('.pdf')
                download_path.rename(pdf_path)
                logger.success(f"Downloaded: {pdf_path.name}")
            else:
                logger.success(f"Downloaded: {download_path.name}")

            return ('success', True)

        except Exception as e:
            logger.error(f"Failed to download {report_filename}: {e}")
            return ('fail', False)

def load_reports_from_csv(csv_path: Path) -> list:
    reports = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            reports.append(row)

    reports.reverse()
    return reports

def setup_aptnotes_repo(data_dir: Path) -> bool:
    """Clone APTnotes repository and create symlink if needed."""
    aptnotes_dir = data_dir / "aptnotes_data"

    # Clone repository if it doesn't exist
    if not aptnotes_dir.exists():
        logger.info("APTnotes repository not found, cloning from GitHub...")
        try:
            subprocess.run(
                ["git", "clone", "https://github.com/aptnotes/data", str(aptnotes_dir)],
                check=True,
                capture_output=True,
                text=True
            )
            logger.success(f"APTnotes repository cloned to {aptnotes_dir}")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to clone APTnotes repository: {e.stderr}")
            return False

    # Create symlink from APTnotes_summary.csv to APTnotes.csv
    csv_summary = aptnotes_dir / "APTnotes_summary.csv"
    csv_link = aptnotes_dir / "APTnotes.csv"

    if csv_summary.exists() and not csv_link.exists():
        logger.info("Creating symlink for APTnotes.csv")
        csv_link.symlink_to("APTnotes_summary.csv")

    return True

async def download_all(reports: list, output: Path, concurrent: int = 10):
    """Download all reports concurrently."""
    semaphore = asyncio.Semaphore(concurrent)
    success_count = 0
    skip_count = 0
    fail_count = 0

    connector = aiohttp.TCPConnector(limit=concurrent)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [download_report(session, report, output, semaphore) for report in reports]

        for coro in asyncio.as_completed(tasks):
            result, _ = await coro
            if result == 'success':
                success_count += 1
            elif result == 'skip':
                skip_count += 1
            else:
                fail_count += 1

    return success_count, skip_count, fail_count

@app.command()
def main(
    csv_path: Annotated[Path, typer.Option(help="Path to APTnotes.csv")] = None,
    output: Annotated[Path, typer.Option(help="Output directory")] = None,
    concurrent: Annotated[int, typer.Option(help="Number of concurrent downloads")] = 10,
):
    logger.info("Starting APTnotes report download from Box.com")

    if csv_path is None:
        csv_path = Config.REPORTS_DIR / "aptnotes_data" / "APTnotes.csv"

    if output is None:
        output = Config.REPORTS_DIR / "aptnotes_pdfs"

    # Setup APTnotes repository if needed
    if not csv_path.exists():
        logger.info("APTnotes data not found, setting up repository...")
        if not setup_aptnotes_repo(Config.REPORTS_DIR):
            logger.error("Failed to setup APTnotes repository")
            raise typer.Exit(code=1)

    if not csv_path.exists():
        logger.error(f"CSV file not found: {csv_path}")
        raise typer.Exit(code=1)

    output.mkdir(parents=True, exist_ok=True)

    logger.info(f"Loading reports from {csv_path}")
    reports = load_reports_from_csv(csv_path)
    logger.info(f"Found {len(reports)} reports to process")

    logger.info(f"Starting downloads with {concurrent} concurrent connections")
    success_count, skip_count, fail_count = asyncio.run(download_all(reports, output, concurrent))

    logger.success("Download complete")
    logger.info(f"Downloaded: {success_count}")
    logger.info(f"Skipped: {skip_count}")
    logger.info(f"Failed: {fail_count}")
    logger.info(f"Total: {len(reports)}")

if __name__ == "__main__":
    app()
